import logging
import requests
import openai

from openai import OpenAI
from utils.retry import retry_with_key_rotation

# ===========================================
# üîπ OpenAI LLM Handler (SDK ‚â• 1.0.0)
# ===========================================
def call_openai(prompt, api_key, model="gpt-3.5-turbo"):
    """
    Calls the OpenAI ChatCompletion API using the new SDK interface.

    Args:
        prompt (str): The user prompt to send to the model.
        api_key (str): The OpenAI API key.
        model (str): The model to use, e.g., gpt-3.5-turbo.

    Returns:
        str: The response text generated by the model.
    """
    logging.info(f"üì° Calling OpenAI | Model: {model}")
    try:
        client = OpenAI(api_key=api_key)

        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
        )
        return response.choices[0].message.content
    except Exception as e:
        logging.exception("‚ùå OpenAI API call failed.")
        raise RuntimeError(f"OpenAI call failed: {e}")

# ===========================================
# üîπ Gemini LLM Handler (Google API)
# ===========================================
def call_gemini(prompt, api_key):
    """
    Calls the Gemini API to generate a response from the model.

    Args:
        prompt (str): The user prompt to send to the model.
        api_key (str): The Gemini API key.

    Returns:
        str: The response text generated by the model.
    """
    try:
        logging.info("üì° Calling Gemini | Model: gemini-pro")
        url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent"
        headers = {"Content-Type": "application/json"}
        payload = {
            "contents": [{"parts": [{"text": prompt}]}]
        }

        response = requests.post(
            url, headers=headers, json=payload, params={"key": api_key}
        )

        if response.status_code != 200:
            logging.error(f"‚ùå Gemini API error: {response.status_code} - {response.text}")
            raise RuntimeError(f"Gemini API error: {response.status_code} - {response.text}")

        return response.json()["candidates"][0]["content"]["parts"][0]["text"]

    except Exception as e:
        logging.exception("‚ùå Gemini API call failed.")
        raise RuntimeError(f"Gemini call failed: {e}")

# ===========================================
# üîπ Unified AI Handler Entry Point
# ===========================================
def get_llm_response(prompt, config):
    """
    Entry point for generating a response from the configured AI provider.

    Args:
        prompt (str): The prompt to send to the model.
        config (dict): Configuration containing AI usage and provider settings.

    Returns:
        str: The AI-generated response, or an empty string if AI is disabled.
    """
    if not config.get("use_ai", False):
        logging.info("‚ö†Ô∏è AI usage is disabled. Skipping LLM call.")
        return ""

    provider = config.get("ai_provider", "openai").lower()
    logging.info(f"üîç Using AI provider: {provider}")

    if provider == "openai":
        model = config.get("openai", {}).get("model", "gpt-3.5-turbo")

        @retry_with_key_rotation("openai", config)
        def call(prompt, api_key=None):
            return call_openai(prompt, api_key=api_key, model=model)

    elif provider == "gemini":
        @retry_with_key_rotation("gemini", config)
        def call(prompt, api_key=None):
            return call_gemini(prompt, api_key=api_key)

    else:
        logging.error(f"‚ùå Unsupported AI provider: {provider}")
        raise ValueError(f"Unsupported AI provider: {provider}")

    return call(prompt)
